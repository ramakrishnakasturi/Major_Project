{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e5c0ca",
   "metadata": {},
   "source": [
    "# Importing neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e9f706",
   "metadata": {
    "executionInfo": {
     "elapsed": 7405,
     "status": "ok",
     "timestamp": 1678949067379,
     "user": {
      "displayName": "Pavan Raju",
      "userId": "08105393391886872316"
     },
     "user_tz": -330
    },
    "id": "90e9f706"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "import extract_features\n",
    "import model\n",
    "import functools\n",
    "import operator\n",
    "import joblib\n",
    "import config\n",
    "import shutil\n",
    "import tqdm\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7d238d",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678949067379,
     "user": {
      "displayName": "Pavan Raju",
      "userId": "08105393391886872316"
     },
     "user_tz": -330
    },
    "id": "cd7d238d"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbb99c",
   "metadata": {},
   "source": [
    "# Functions to extract features from the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404b3c79",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678949067380,
     "user": {
      "displayName": "Pavan Raju",
      "userId": "08105393391886872316"
     },
     "user_tz": -330
    },
    "id": "404b3c79"
   },
   "outputs": [],
   "source": [
    "def video_to_frames(video):\n",
    "    path = os.path.join(config.test_path, 'temporary_images')\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path)\n",
    "    video_path = \"Test/TestVideo/\"\n",
    "    count = 0\n",
    "    image_list = []\n",
    "    # Path to video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret is False:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(config.test_path, 'temporary_images', 'frame%d.jpg' % count), frame)\n",
    "        image_list.append(os.path.join(config.test_path, 'temporary_images', 'frame%d.jpg' % count))\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return image_list\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    return img\n",
    "\n",
    "\n",
    "def model_vgg_feature_load():\n",
    "    model = VGG16(weights=\"imagenet\", include_top=True, input_shape=(224, 224, 3))\n",
    "    out = model.layers[-2].output\n",
    "    model_final = Model(inputs=model.input, outputs=out)\n",
    "    return model_final\n",
    "\n",
    "\n",
    "def extract_feats_pretrained_cnn():\n",
    "    \"\"\"\n",
    "    saves the numpy features from all the videos\n",
    "    \"\"\"\n",
    "    model = model_vgg_feature_load()\n",
    "    print('Model loaded')\n",
    "\n",
    "    if not os.path.isdir(os.path.join(config.test_path, 'feat')):\n",
    "        os.mkdir(os.path.join(config.test_path, 'feat'))\n",
    "\n",
    "    video_list = os.listdir(os.path.join(config.test_path, 'video'))\n",
    "    \n",
    "    #Ù‹When running the script on Colab an item called '.ipynb_checkpoints' \n",
    "    #is added to the beginning of the list causing errors later on, so the next line removes it.\n",
    "    #video_list.remove('.ipynb_checkpoints')\n",
    "    \n",
    "    for video in video_list:\n",
    "\n",
    "        outfile = os.path.join(config.test_path, 'feat', video + '.npy')\n",
    "        img_feats = extract_features(video, model)\n",
    "        np.save(outfile, img_feats)\n",
    "        \n",
    "        \n",
    "        \n",
    "def define_discriminator():\n",
    "    word_vector_dim = 300\n",
    "    dropout_prob = 0.4\n",
    "\n",
    "    in_label = layers.Input(shape=(300,))\n",
    "\n",
    "    n_nodes = 3 * 64 * 64\n",
    "    li = layers.Dense(n_nodes)(in_label)\n",
    "    li = layers.Reshape((64, 64, 3))(li)\n",
    "\n",
    "    dis_input = layers.Input(shape=(64, 64, 3))\n",
    "\n",
    "    merge = layers.Concatenate()([dis_input, li])\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(merge)\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "    discriminator = layers.GaussianNoise(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(discriminator)\n",
    "    discriminator = layers.BatchNormalization(momentum=0.5)(discriminator)\n",
    "    discriminator = layers.LeakyReLU()(discriminator)\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(discriminator)\n",
    "    discriminator = layers.BatchNormalization(momentum=0.5)(discriminator)\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(discriminator)\n",
    "    discriminator = layers.BatchNormalization(momentum=0.5)(discriminator)\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(discriminator)\n",
    "    discriminator = layers.BatchNormalization(momentum=0.5)(discriminator)\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(discriminator)\n",
    "    discriminator = layers.BatchNormalization(momentum=0.5)(discriminator)\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(discriminator)\n",
    "    discriminator = layers.BatchNormalization(momentum=0.5)(discriminator)\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Flatten()(discriminator)\n",
    "\n",
    "    discriminator = layers.Dense(1024)(discriminator)\n",
    "\n",
    "    discriminator = layers.LeakyReLU(0.2)(discriminator)\n",
    "\n",
    "    discriminator = layers.Dense(1)(discriminator)\n",
    "\n",
    "    discriminator_model = Model(inputs=[dis_input, in_label], outputs=discriminator)\n",
    "\n",
    "    discriminator_model.summary()\n",
    "\n",
    "    return discriminator_model\n",
    "\n",
    "\n",
    "def resnet_block(model, kernel_size, filters, strides):\n",
    "    gen = model\n",
    "    model = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=\"same\")(model)\n",
    "    model = layers.BatchNormalization(momentum=0.5)(model)\n",
    "    model = tf.keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1, 2])(model)\n",
    "    model = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=\"same\")(model)\n",
    "    model = layers.BatchNormalization(momentum=0.5)(model)\n",
    "    model = layers.Add()([gen, model])\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_generator():\n",
    "    kernel_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    batch_init = tf.random_normal_initializer(1., 0.02)\n",
    "\n",
    "    random_input = layers.Input(shape=(100,))\n",
    "    text_input1 = layers.Input(shape=(300,))\n",
    "    text_layer1 = layers.Dense(8192)(text_input1)\n",
    "    text_layer1 = layers.Reshape((8, 8, 128))(text_layer1)\n",
    "\n",
    "    n_nodes = 128 * 8 * 8\n",
    "    gen_input_dense = layers.Dense(n_nodes)(random_input)\n",
    "    generator = layers.Reshape((8, 8, 128))(gen_input_dense)\n",
    "\n",
    "    merge = layers.Concatenate()([generator, text_layer1])\n",
    "\n",
    "    model = layers.Conv2D(filters=64, kernel_size=9, strides=1, padding=\"same\")(merge)\n",
    "    model = tf.keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1, 2])(model)\n",
    "\n",
    "    gen_model = model\n",
    "\n",
    "    for _ in range(4):\n",
    "        model = resnet_block(model, 3, 64, 1)\n",
    "\n",
    "    model = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(model)\n",
    "    model = layers.BatchNormalization(momentum=0.5)(model)\n",
    "    model = layers.Add()([gen_model, model])\n",
    "\n",
    "    model = layers.Conv2DTranspose(filters=512, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=kernel_init)(model)\n",
    "    model = layers.LeakyReLU(0.2)(model)\n",
    "\n",
    "    model = layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=kernel_init)(model)\n",
    "    model = layers.LeakyReLU(0.2)(model)\n",
    "\n",
    "    model = layers.Conv2DTranspose(filters=128, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=kernel_init)(model)\n",
    "    model = layers.LeakyReLU(0.2)(model)\n",
    "\n",
    "    model = layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_init)(model)\n",
    "    model = layers.LeakyReLU(0.2)(model)\n",
    "\n",
    "    model = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(model)\n",
    "\n",
    "    generator_model = Model(inputs=[random_input, text_input1], outputs=model)\n",
    "\n",
    "    generator_model.summary()\n",
    "\n",
    "    return generator_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c791187",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678949067380,
     "user": {
      "displayName": "Pavan Raju",
      "userId": "08105393391886872316"
     },
     "user_tz": -330
    },
    "id": "5c791187"
   },
   "outputs": [],
   "source": [
    "class VideoDescriptionRealTime(object):\n",
    "    \"\"\"\n",
    "        Initialize the parameters for the model\n",
    "        \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.latent_dim = config.latent_dim\n",
    "        self.num_encoder_tokens = config.num_encoder_tokens\n",
    "        self.num_decoder_tokens = config.num_decoder_tokens\n",
    "        self.time_steps_encoder = config.time_steps_encoder\n",
    "        self.max_probability = config.max_probability\n",
    "\n",
    "        # models\n",
    "        self.tokenizer, self.inf_encoder_model, self.inf_decoder_model = model.inference_model()\n",
    "        #self.inf_decoder_model = None\n",
    "        self.save_model_path = config.save_model_path\n",
    "        self.test_path = config.test_path\n",
    "        self.search_type = config.search_type\n",
    "        self.num = 0\n",
    "\n",
    "    def greedy_search(self, loaded_array):\n",
    "        \"\"\"\n",
    "\n",
    "        :param f: the loaded numpy array after creating videos to frames and extracting features\n",
    "        :return: the final sentence which has been predicted greedily\n",
    "        \"\"\"\n",
    "        inv_map = self.index_to_word()\n",
    "        states_value = self.inf_encoder_model.predict(loaded_array.reshape(-1, 80, 4096))\n",
    "        target_seq = np.zeros((1, 1, 1500))\n",
    "        final_sentence = ''\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        for i in range(15):\n",
    "            output_tokens, h, c = self.inf_decoder_model.predict([target_seq] + states_value)\n",
    "            states_value = [h, c]\n",
    "            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
    "            y_hat = np.argmax(output_tokens)\n",
    "            if y_hat == 0:\n",
    "                continue\n",
    "            if inv_map[y_hat] is None:\n",
    "                break\n",
    "            if inv_map[y_hat] == 'eos':\n",
    "                break\n",
    "            else:\n",
    "                final_sentence = final_sentence + inv_map[y_hat] + ' '\n",
    "                target_seq = np.zeros((1, 1, 1500))\n",
    "                target_seq[0, 0, y_hat] = 1\n",
    "        return final_sentence\n",
    "\n",
    "    def decode_sequence2bs(self, input_seq):\n",
    "        states_value = self.inf_encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        self.beam_search(target_seq, states_value, [], [], 0)\n",
    "        return decode_seq\n",
    "\n",
    "    def beam_search(self, target_seq, states_value, prob, path, lens):\n",
    "        \"\"\"\n",
    "\n",
    "        :param target_seq: the array that is fed into the model to predict the next word\n",
    "        :param states_value: previous state that is fed into the lstm cell\n",
    "        :param prob: probability of predicting a word\n",
    "        :param path: list of words from each sentence\n",
    "        :param lens: number of words\n",
    "        :return: final sentence\n",
    "        \"\"\"\n",
    "        global decode_seq\n",
    "        node = 2\n",
    "        output_tokens, h, c = self.inf_decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
    "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
    "        states_value = [h, c]\n",
    "        for i in range(node):\n",
    "            if sampled_token_index[i] == 0:\n",
    "                sampled_char = ''\n",
    "            else:\n",
    "                sampled_char = list(self.tokenizer.word_index.keys())[\n",
    "                    list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
    "            MAX_LEN = 12\n",
    "            if sampled_char != 'eos' and lens <= MAX_LEN:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                if sampled_char == '':\n",
    "                    p = 1\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                path_new = list(path)\n",
    "                path_new.append(sampled_char)\n",
    "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
    "                self.beam_search(target_seq, states_value, prob_new, path_new, lens + 1)\n",
    "            else:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                p = functools.reduce(operator.mul, prob_new, 1)\n",
    "                if p > self.max_probability:\n",
    "                    decode_seq = path\n",
    "                    self.max_probability = p\n",
    "\n",
    "    def decoded_sentence_tuning(self, decoded_sentence):\n",
    "        # tuning sentence\n",
    "        decode_str = []\n",
    "        filter_string = ['bos', 'eos']\n",
    "        uni_gram = {}\n",
    "        last_string = \"\"\n",
    "        for idx2, c in enumerate(decoded_sentence):\n",
    "            if c in uni_gram:\n",
    "                uni_gram[c] += 1\n",
    "            else:\n",
    "                uni_gram[c] = 1\n",
    "            if last_string == c and idx2 > 0:\n",
    "                continue\n",
    "            if c in filter_string:\n",
    "                continue\n",
    "            if len(c) > 0:\n",
    "                decode_str.append(c)\n",
    "            if idx2 > 0:\n",
    "                last_string = c\n",
    "        return decode_str\n",
    "\n",
    "    def index_to_word(self):\n",
    "        # inverts word tokenizer\n",
    "        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n",
    "        return index_to_word\n",
    "\n",
    "    def get_test_data(self):\n",
    "        # loads the features array\n",
    "        file_list = os.listdir(os.path.join(self.test_path,\"video\"))\n",
    "        print(file_list)\n",
    "        try:\n",
    "            file_list.remove('.DS_Store')\n",
    "        print(file_list)\n",
    "        # with open(os.path.join(self.test_path, 'testing.txt')) as testing_file:\n",
    "            # lines = testing_file.readlines()\n",
    "        # file_name = lines[self.num].strip()\n",
    "        file_name = file_list[self.num]\n",
    "        path = os.path.join(self.test_path, 'feat', file_name + '.npy')\n",
    "        if os.path.exists(path):\n",
    "            f = np.load(path)\n",
    "        else:\n",
    "            model = extract_features.model_cnn_load()\n",
    "            f = extract_features.extract_features(file_name, model)\n",
    "        if self.num < len(file_list):\n",
    "            self.num += 1\n",
    "        else:\n",
    "            self.num = 0\n",
    "        return f, file_name\n",
    "\n",
    "    def test(self):\n",
    "        X_test, filename = self.get_test_data()\n",
    "        # generate inference test outputs\n",
    "        if self.search_type == 'greedy':\n",
    "            sentence_predicted = self.greedy_search(X_test.reshape((-1, 80, 4096)))\n",
    "        else:\n",
    "            sentence_predicted = ''\n",
    "            decoded_sentence = self.decode_sequence2bs(X_test.reshape((-1, 80, 4096)))\n",
    "            decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
    "            for d in decode_str:\n",
    "                sentence_predicted = sentence_predicted + d + ' '\n",
    "        # re-init max prob\n",
    "        self.max_probability = -1\n",
    "        return sentence_predicted, filename\n",
    "\n",
    "    def main(self, filename, caption):\n",
    "        \"\"\"\n",
    "\n",
    "        :param filename: the video to load\n",
    "        :param caption: final caption\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 1. Initialize reading video object\n",
    "        cap1 = cv2.VideoCapture(os.path.join(self.test_path, 'video', filename))\n",
    "        cap2 = cv2.VideoCapture(os.path.join(self.test_path, 'video', filename))\n",
    "        caption = '[' + ' '.join(caption.split()[1:]) + ']'\n",
    "        # 2. Cycle through pictures\n",
    "        while cap1.isOpened():\n",
    "            ret, frame = cap2.read()\n",
    "            ret2, frame2 = cap1.read()\n",
    "            if ret:\n",
    "                imS = cv2.resize(frame, (480, 300))\n",
    "                cv2.putText(imS, caption, (100, 270), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0),\n",
    "                            2, cv2.LINE_4)\n",
    "                cv2.imshow(\"VIDEO CAPTIONING\", imS)\n",
    "            if ret2:\n",
    "                imS = cv2.resize(frame, (480, 300))\n",
    "                cv2.imshow(\"ORIGINAL\", imS)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # Quit playing\n",
    "            key = cv2.waitKey(25)\n",
    "            if key == 27:  # Button esc\n",
    "                break\n",
    "\n",
    "        # 3. Free resources\n",
    "        cap1.release()\n",
    "        cap2.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7dd4f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "executionInfo": {
     "elapsed": 15655,
     "status": "error",
     "timestamp": 1678949157913,
     "user": {
      "displayName": "Pavan Raju",
      "userId": "08105393391886872316"
     },
     "user_tz": -330
    },
    "id": "9d7dd4f2",
    "outputId": "5f369007-9839-4538-d7be-8353d69875fe",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:02:59.784151: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-16 13:02:59.784169: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      ".........................\n",
      "Generating Caption:\n",
      "\n",
      "['.DS_Store', '_6OTzzK7t9Y_158_170.avi', '_0nX-El-ySo_83_93.avi', '_1vy2HIN60A_32_40.avi', '_7nP9z6T9m8_11_17.avi', '_9iG5Ge01PM_3_11.avi', '_6OTzzK7t9Y_73_78.avi', 'test.avi']\n",
      "\n",
      "Processing video .DS_Store\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Images List : []\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"Video Captioning/testing_video/video/.DS_Store\"\n",
      "[ERROR:0@2.487] global cap.cpp:166 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): Video Captioning/testing_video/video/.DS_Store in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.........................\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating Caption:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m video_caption, file \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_to_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 149\u001b[0m, in \u001b[0;36mVideoDescriptionRealTime.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# generate inference test outputs\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 149\u001b[0m     sentence_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m4096\u001b[39m)))\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     sentence_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_to_text = VideoDescriptionRealTime(config)\n",
    "    while True:\n",
    "        print('.........................\\nGenerating Caption:\\n')\n",
    "        start = time.time()\n",
    "        video_caption, file = video_to_text.test()\n",
    "        end = time.time()\n",
    "        sentence = ''\n",
    "        print(sentence)\n",
    "        for text in video_caption.split():\n",
    "            sentence = sentence + ' ' + text\n",
    "        print('\\n.........................\\n')\n",
    "        print(sentence)\n",
    "        print('\\n.........................\\n')\n",
    "        print('It took {:.2f} seconds to generate caption'.format(end-start))\n",
    "        video_to_text.main(file, sentence)\n",
    "        play_video = input('Should I play the video? ')\n",
    "        if play_video.lower() == 'y':\n",
    "            continue\n",
    "        elif play_video.lower() == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print('Could not understand type (y) for yes and (n) for no')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8f286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
